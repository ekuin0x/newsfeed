<!DOCTYPE html>
<html lang="en">

<head>
        <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-5QC942MV32"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-5QC942MV32');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Brain’s Role in Social Cues</title>
    <script src="https://code.jquery.com/jquery-3.7.1.min.js"
        integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.2/font/bootstrap-icons.min.css">
    <meta name="description" content="Summary: Researchers uncovered a crucial role of the ventrolateral prefrontal co">
    <link rel="stylesheet" href="/static/style.css">
</head>

<body onload="related()">
    <nav>
        <div id="logo">NewsFeed</div>
        <a href="/a/business">Business</a>
        <a href="/a/world">World</a>
        <a href="/a/technology">Technology</a>
        <a href="/a/health">Health</a>
        <a href="/a/entertainment">Entertainment</a>
        <i class="bi bi-list" onclick="openNav()"></i>
        <i class="bi bi-search" onclick="openSearch()"></i>
    </nav>

    <div id="mobile-nav">
        <i class="bi bi-x-lg" onclick="closeNav()"></i>
        <a href="/a/business">Business</a>
        <a href="/a/world">World</a>
        <a href="/a/technology">Technology</a>
        <a href="/a/health">Health</a>
        <a href="/a/entertainment">Entertainment</a>
    </div>

    <form action="/search" method="get" id="search">
        <input type="text" name="q" />
        <i class="bi bi-search" onclick="search()"></i>
    </form>

    <div id="wrapper">

        <div class="flex1" id="article">
            <p>Brain’s Role in Social Cues</p>
            <span id="topic" >health</span>
                <span class="time">
                    <i class="bi bi-clock"></i><b>Nov-28, 2023</b>
                </span>
                <br><br>
                <img src="https://neurosciencenews.com/files/2023/11/vmpfc-social-cues-neuroscience.jpg" alt="">

                <textarea id="text" readonly> Summary: Researchers uncovered a crucial role of the ventrolateral prefrontal cortex (VLPFC) in processing social cues, expanding our understanding of this brain region beyond working memory and multisensory integration.

Using a novel approach, the study found that while individual neurons in the VLPFC of macaques showed complex responses to social stimuli, their collective activity could be decoded by a machine learning model to identify expressions and identities in videos.

This discovery suggests that the VLPFC plays a significant role in integrating facial and vocal information, crucial for social communication.

The findings provide insights into how the brain processes social information, which may be altered in communication disorders like autism.

Key Facts:

The study recorded over 400 neurons in the VLPFC of macaques, revealing that while single neurons had complex responses, their collective activity could decode social cues in videos. This research indicates the VLPFC’s critical role in social communication by integrating facial expressions, vocalizations, and identities. The findings could aid in understanding speech and communication disorders, where the integration of multisensory stimuli may be compromised.

Source: University of Rochester

Researchers have discovered that a part of the brain associated with working memory and multisensory integration may also play an important role in how the brain processes social cues.

Previous research has shown that neurons in the ventrolateral prefrontal cortex (VLPFC) integrate faces and voices—but new research, in the Journal of Neuroscience, shows that neurons in the VLPFC play a role in processing both the identity of the “speaker” and the expression conveyed by facial gestures and vocalizations.

“We still don’t fully understand how facial and vocal information is combined and what information is processed by different brain regions,” said Lizabeth Romanski, PhD, associate professor of Neuroscience at the Del Monte Institute for Neuroscience at the University of Rochester and senior author of the study.

“However, these findings confirm VLPFC as a critical node in the social communication network that processes facial expressions, vocalizations, and social cues.”

The VLPFC is an area of the brain that is enlarged in primates, including humans and macaques. In this study, the Romanski Lab showed rhesus macaques short videos of other macaques engaging in vocalizations/expressions that were friendly, aggressive, or neutral.

They recorded the activity of more than 400 neurons in the VLPFC and found that individually, the cells did not exhibit strong categorical responses to the expressions or the identities of the macaques in the videos.

However, when the researchers combined the neurons as a population a machine learning model could be trained to decode the expression and identity in the videos based only on the patterns of neural activity, suggesting that neurons were collectively responding to these variables.

Overall, the activity of the population of VLPFC neurons was primarily dictated by the identity of the macaque in the video. These findings suggest that the VLPFC is a key brain region in the processing of social cues.

“We used dynamic, information-rich stimuli in our study and the responses we saw from single neurons were very complex. Initially, it was difficult to make sense of the data,” said Keshov Sharma, PhD, lead author on the study.

“It wasn’t until we studied how population activity correlated with the social information in our stimuli that we found a coherent structure. For us, it was like finally seeing a forest instead of a muddle of trees.”

Sharma and Romanski hope their approach will encourage others to analyze population-level activity when studying how faces and voices are integrated in the brain.

Dissecting the brain’s audiovisual processing circuits

Understanding how the prefrontal cortex processes auditory and visual information is a cornerstone of the Romanski lab. This process is necessary for recognizing objects by sight, as well as sound, and is required for effective communication.

In previous research, the Romanski Lab identified the VLPFC as an area of the brain responsible for maintaining and integrating face and vocal information during working memory. This body of research points to the importance of this brain region within the larger circuit that underlies social communication.

“Knowing what features populations of neurons extract from face and vocal stimuli and how these features are typically integrated will help us to understand what may be altered in speech and communication disorders, including autism spectrum disorders, where multiple sensory stimuli may not combine optimally,” Romanski said.

Additional authors include Mark Diltz of the University of Rochester Medical Center, Theodore Lincoln of Astrobotic Technology Inc., and Eric Albuquerque of the University of Miami School of Medicine.

Funding: This research was supported by the National Institutes of Health, the Schmitt Program for Integrative Neuroscience through the Del Monte Institute for Neuroscience, and the University of Rochester Medical Scientist Training Program (MSTP).

About this social neuroscience research news

Author: Kelsie Smith Hayduk

Source: University of Rochester

Contact: Kelsie Smith Hayduk – University of Rochester

Image: The image is credited to Neuroscience News

Original Research: Closed access.

“Neuronal Population Encoding of Identity in Primate Prefrontal Cortex” by Lizabeth Romanski, et al. Journal of Neuroscience

Abstract

Neuronal Population Encoding of Identity in Primate Prefrontal Cortex

The ventrolateral prefrontal cortex (VLPFC) shows robust activation during the perception of faces and voices. However, little is known about what categorical features of social stimuli drive neural activity in this region.

Since perception of identity and expression are critical social functions, we examined whether neural responses to naturalistic stimuli were driven by these two categorial features in the prefrontal cortex.

We recorded single neurons in the VLPFC, while two male rhesus macaques (M. Mulatta) viewed short audiovisual videos of unfamiliar conspecifics making expressions of aggressive, affiliative, and neutral valence.

Of the 285 neurons responsive to the audiovisual stimuli, 111 neurons had a main effect (two-way ANOVA) of identity, expression or their interaction in their stimulus related firing rates; however, decoding of expression and identity using single unit firing rates rendered poor accuracy.

Interestingly, when decoding from pseudopopulations of recorded neurons, the accuracy for both expression and identity increased with population size, suggesting that the population transmitted information relevant to both variables.

Principal components analysis of mean population activity across time revealed that population responses to the same identity followed similar trajectories in the response space, facilitating segregation from other identities.

Our results suggest that identity is a critical feature of social stimuli that dictates the structure of population activity in the VLPFC, during the perception of vocalizations and their corresponding facial expressions. These findings enhance our understanding of the role of the VLPFC in social behavior. </textarea>

        </div>



        <div class="flex2">

            <header>RELATED NEWS</header>

            <div id="related"></div>


        </div>

    </div>

    <script src="/static/script.js"> </script>


</body>

</html>